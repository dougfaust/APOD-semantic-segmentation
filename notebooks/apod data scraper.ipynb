{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc34bdb",
   "metadata": {},
   "source": [
    "## Notebook to scrape, download, and preprocess APOD images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd73bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb518c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['planetary_nebula', 'comet', 'reflection_nebula', 'spiral_galaxy', 'aurora_borealis']\n",
    "\n",
    "files = glob.glob('*.html')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff1bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_from_daily_page(link):\n",
    "    \"\"\" get full-sized APOD image from daily page \"\"\"\n",
    "    r = requests.get(link)\n",
    "\n",
    "    im_soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    \n",
    "    url_stem = 'https://apod.nasa.gov/apod/'\n",
    "    \n",
    "    pic_url = url_stem + im_soup('img')[0].attrs['src']\n",
    "    \n",
    "    response = requests.get(pic_url)\n",
    "    \n",
    "    return Image.open(BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd7dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_urls_from_apod_search_page(html_file):\n",
    "    \"\"\" parses html for APOD search results page to get full-sized image urls \"\"\"\n",
    "    soup = BeautifulSoup(open(url, encoding=\"utf8\"), \"html.parser\")\n",
    "\n",
    "    img_links = [x.find_previous('a') for x in soup('img')]\n",
    "    \n",
    "    img_refs = [x['href'] for x in img_links if x]\n",
    "    \n",
    "    print(f\"{len(img_refs)} image links recovered\")\n",
    "    \n",
    "    return img_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32cd82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_data_from_links(img_links, class_name, base_path, test_ratio=0.7):\n",
    "    \"\"\" downloads, preprocesses and stores images into architecture \"\"\"\n",
    "    \n",
    "    # create train/class and test/class pathways\n",
    "    train_path = Path(base_path + \"/train/\" + class_name)\n",
    "    test_path = Path(base_path + \"/test/\" + class_name)\n",
    "    \n",
    "    try:\n",
    "        train_path.mkdir(parents=True, exist_ok=False)\n",
    "        test_path.mkdir(parents=True, exist_ok=False)\n",
    "    except FileExistsError:\n",
    "        print(f\"paths for this {class_name} already exist\")\n",
    "    else:\n",
    "        print(f\"paths for {class_name} class have been created\")\n",
    "    \n",
    "    # create train test split\n",
    "    train_mask = np.full(len(img_links), True)\n",
    "    train_mask[round(test_ratio*len(img_links)):] = False\n",
    "    random.shuffle(train_mask)\n",
    "    \n",
    "    for is_train, link in zip(train_mask, img_links):\n",
    "        # download image\n",
    "        image = get_image_from_daily_page(link)\n",
    "        \n",
    "        #process image\n",
    "        image = preprocess_image(image)\n",
    "    \n",
    "        # store processed image\n",
    "        if is_train:\n",
    "            img.save(train_path.as_posix() + '/' + link.split('/')[-1].split('.')[0] + '.jpg')\n",
    "        else:\n",
    "            img.save(test_path.as_posix() + '/' + link.split('/')[-1].split('.')[0] + '.jpg')\n",
    "\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\" either done here or in model training \"\"\"\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0556776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_label, file in zip(class_names, files):\n",
    "    img_urls = get_img_urls_from_apod_search_page(file)\n",
    "    \n",
    "    filepath = \"/home/doug/Projects/apod_semantic_segmentation/data\"\n",
    "    create_class_data_from_links(img_urls, class_name=class_label, base_path=filepath, test_ratio=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
