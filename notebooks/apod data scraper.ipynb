{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc34bdb",
   "metadata": {},
   "source": [
    "## Notebook to scrape, download, and preprocess APOD images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00dd73bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb518c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class names (i.e. labels) in the same order as the search results files in the glob\n",
    "class_names = ['planetary_nebula', 'comet', 'reflection_nebula', 'aurora', 'spiral_galaxy']\n",
    "\n",
    "files = glob.glob('*.html')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff1bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_from_daily_page(link):\n",
    "    \"\"\" get full-sized APOD image from daily page \"\"\"\n",
    "    \n",
    "    r = requests.get(link)\n",
    "\n",
    "    im_soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    \n",
    "    url_stem = 'https://apod.nasa.gov/apod/'\n",
    "    \n",
    "    if len(im_soup('img')) > 0:\n",
    "        pic_url = url_stem + im_soup('img')[0].attrs['src']\n",
    "    else: \n",
    "        return None\n",
    "    \n",
    "    response = requests.get(pic_url)\n",
    "    \n",
    "    return Image.open(BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dea660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_urls_from_apod_search_page(html_file):\n",
    "    \"\"\" parses html for APOD search results page to get full-sized image urls \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(open(html_file, encoding=\"utf8\"), \"html.parser\")\n",
    "\n",
    "    img_links = [x.find_previous('a') for x in soup('img')]\n",
    "    \n",
    "    img_refs = [x['href'] for x in img_links if x]\n",
    "    \n",
    "    print(f\"{len(img_refs)} image links recovered\")\n",
    "    \n",
    "    return img_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32cd82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_data_from_links(img_links, class_name, base_path, test_ratio=0.7):\n",
    "    \"\"\" downloads, preprocesses and stores images into architecture \"\"\"\n",
    "    \n",
    "    # create train/class and test/class pathways\n",
    "    train_path = Path(base_path + \"/train/\" + class_name)\n",
    "    test_path = Path(base_path + \"/test/\" + class_name)\n",
    "    \n",
    "    try:\n",
    "        train_path.mkdir(parents=True, exist_ok=False)\n",
    "        test_path.mkdir(parents=True, exist_ok=False)\n",
    "    except FileExistsError:\n",
    "        print(f\"paths for this {class_name} already exist\")\n",
    "    else:\n",
    "        print(f\"paths for {class_name} class have been created\")\n",
    "    \n",
    "    # create train test split\n",
    "    train_mask = np.full(len(img_links), True)\n",
    "    train_mask[round(test_ratio*len(img_links)):] = False\n",
    "    random.shuffle(train_mask)\n",
    "    \n",
    "    for is_train, link in zip(train_mask, img_links):\n",
    "        # download image\n",
    "        image = get_image_from_daily_page(link)\n",
    "        if not image: continue \n",
    "        \n",
    "        #process image\n",
    "        image = preprocess_image(image)\n",
    "    \n",
    "        # store processed image in data/train/classname or data/test/classname format\n",
    "        if is_train:\n",
    "            image.save(train_path.as_posix() + '/' + link.split('/')[-1].split('.')[0] + '.PNG')\n",
    "        else:\n",
    "            image.save(test_path.as_posix() + '/' + link.split('/')[-1].split('.')[0] + '.PNG')\n",
    "\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\" either done here or in model training \"\"\"\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47ddbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_label, file in zip(class_names[1:], files[1:]):\n",
    "    img_urls = get_img_urls_from_apod_search_page(file)\n",
    "    \n",
    "    # file path for data, should have train and test subfolders\n",
    "    filepath = \"/home/doug/Projects/apod_semantic_segmentation/data\"\n",
    "    \n",
    "    create_class_data_from_links(img_urls, class_name=class_label, base_path=filepath, test_ratio=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4de461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_through_dir(dir_path):\n",
    "  \"\"\"Walks through dir_path and returns its contents.\"\"\"\n",
    "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1af61c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 directories and 0 images in '/home/doug/Projects/apod_semantic_segmentation/data/train'.\n",
      "There are 0 directories and 139 images in '/home/doug/Projects/apod_semantic_segmentation/data/train/spiral_galaxy'.\n",
      "There are 0 directories and 140 images in '/home/doug/Projects/apod_semantic_segmentation/data/train/planetary_nebula'.\n",
      "There are 0 directories and 136 images in '/home/doug/Projects/apod_semantic_segmentation/data/train/aurora'.\n",
      "There are 0 directories and 140 images in '/home/doug/Projects/apod_semantic_segmentation/data/train/reflection_nebula'.\n",
      "There are 0 directories and 135 images in '/home/doug/Projects/apod_semantic_segmentation/data/train/comet'.\n"
     ]
    }
   ],
   "source": [
    "walk_through_dir(\"/home/doug/Projects/apod_semantic_segmentation/data/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8af3f5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 directories and 0 images in '/home/doug/Projects/apod_semantic_segmentation/data/test'.\n",
      "There are 0 directories and 60 images in '/home/doug/Projects/apod_semantic_segmentation/data/test/spiral_galaxy'.\n",
      "There are 0 directories and 60 images in '/home/doug/Projects/apod_semantic_segmentation/data/test/planetary_nebula'.\n",
      "There are 0 directories and 59 images in '/home/doug/Projects/apod_semantic_segmentation/data/test/aurora'.\n",
      "There are 0 directories and 60 images in '/home/doug/Projects/apod_semantic_segmentation/data/test/reflection_nebula'.\n",
      "There are 0 directories and 59 images in '/home/doug/Projects/apod_semantic_segmentation/data/test/comet'.\n"
     ]
    }
   ],
   "source": [
    "walk_through_dir(\"/home/doug/Projects/apod_semantic_segmentation/data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8dd796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
